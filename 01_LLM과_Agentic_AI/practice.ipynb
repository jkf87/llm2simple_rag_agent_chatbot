{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-2교시: LLM의 원리와 한계, Agentic AI 구조 이해\n",
    "\n",
    "## 실습 목표\n",
    "- LLM API를 직접 호출하여 환각 현상 체험\n",
    "- 지식 단절 문제 확인\n",
    "- Python 기본 구조 이해\n",
    "\n",
    "## 사용 모델\n",
    "- **Google Gemini 2.5 Flash Lite** (무료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Python 기본 구조 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 변수와 자료형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 자료형\n",
    "name = \"Gemini\"          # 문자열 (str)\n",
    "version = 2.0            # 실수 (float)\n",
    "token_limit = 1000000    # 정수 (int)\n",
    "is_free = True           # 불린 (bool)\n",
    "\n",
    "print(f\"모델명: {name}\")\n",
    "print(f\"버전: {version}\")\n",
    "print(f\"토큰 한도: {token_limit:,}\")\n",
    "print(f\"무료인가?: {is_free}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 리스트와 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 - 순서가 있는 데이터 모음\n",
    "llm_models = [\"GPT-4\", \"Claude\", \"Gemini\", \"LLaMA\"]\n",
    "print(\"LLM 모델들:\", llm_models)\n",
    "print(\"첫 번째 모델:\", llm_models[0])\n",
    "\n",
    "# 딕셔너리 - 키-값 쌍의 데이터\n",
    "model_info = {\n",
    "    \"name\": \"Gemini\",\n",
    "    \"company\": \"Google\",\n",
    "    \"release_year\": 2024,\n",
    "    \"features\": [\"대화\", \"코딩\", \"분석\", \"멀티모달\"]\n",
    "}\n",
    "print(\"\\n모델 정보:\", model_info)\n",
    "print(\"모델 이름:\", model_info[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 함수 정의와 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 정의\n",
    "def greet_user(name, role=\"학생\"):\n",
    "    \"\"\"\n",
    "    사용자에게 인사하는 함수\n",
    "    \n",
    "    Args:\n",
    "        name: 사용자 이름\n",
    "        role: 사용자 역할 (기본값: 학생)\n",
    "    \n",
    "    Returns:\n",
    "        인사 메시지\n",
    "    \"\"\"\n",
    "    return f\"안녕하세요, {role} {name}님! AI 수업에 오신 것을 환영합니다.\"\n",
    "\n",
    "# 함수 호출\n",
    "message = greet_user(\"홍길동\")\n",
    "print(message)\n",
    "\n",
    "message2 = greet_user(\"김선생\", role=\"선생님\")\n",
    "print(message2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 조건문과 반복문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조건문\n",
    "temperature = 0.7\n",
    "\n",
    "if temperature < 0.3:\n",
    "    creativity = \"매우 보수적\"\n",
    "elif temperature < 0.7:\n",
    "    creativity = \"적당한 균형\"\n",
    "else:\n",
    "    creativity = \"매우 창의적\"\n",
    "\n",
    "print(f\"Temperature {temperature}: {creativity}\")\n",
    "\n",
    "# 반복문\n",
    "print(\"\\nLLM 모델 목록:\")\n",
    "for i, model in enumerate(llm_models, 1):\n",
    "    print(f\"  {i}. {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "!pip install google-generativeai python-dotenv requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 환경변수 로드 (있는 경우)\n",
    "load_dotenv()\n",
    "\n",
    "print(\"라이브러리 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key 설정\n",
    "\n",
    "Google AI Studio에서 무료 API Key를 발급받으세요:\n",
    "- https://aistudio.google.com/app/apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31merror: failed-wheel-build-for-install\n",
      "\u001b[1;31m\n",
      "\u001b[1;31m× Failed to build installable wheels for some pyproject.toml based projects\n",
      "\u001b[1;31m╰─> pyzmq. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 방법 1: 직접 입력 (실습용)\n",
    "# GOOGLE_API_KEY = \"your-api-key-here\"\n",
    "\n",
    "# 방법 2: 환경변수에서 로드 (권장)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "\n",
    "# 방법 3: Colab에서 입력받기\n",
    "if not GOOGLE_API_KEY:\n",
    "    from getpass import getpass\n",
    "    GOOGLE_API_KEY = getpass(\"Google API Key를 입력하세요: \")\n",
    "\n",
    "# Gemini API 설정\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 모델 생성\n",
    "model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
    "\n",
    "print(\"Gemini API 설정 완료!\")\n",
    "print(\"사용 모델: gemini-2.5-flash-lite (무료)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. API 호출 기본 구조 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 HTTP 요청의 기본 개념\n",
    "\n",
    "API(Application Programming Interface)는 프로그램 간 소통하는 방법입니다.\n",
    "\n",
    "```\n",
    "┌─────────────┐     HTTP 요청      ┌─────────────┐\n",
    "│   클라이언트  │ ───────────────→ │    서버      │\n",
    "│  (우리 코드)  │                  │  (Google)   │\n",
    "│             │ ←─────────────── │             │\n",
    "└─────────────┘     HTTP 응답      └─────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 requests 라이브러리로 API 호출 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 GET 요청 예시 (공공 API)\n",
    "response = requests.get(\"https://jsonplaceholder.typicode.com/posts/1\")\n",
    "\n",
    "print(\"상태 코드:\", response.status_code)  # 200이면 성공\n",
    "print(\"\\n응답 데이터:\")\n",
    "print(json.dumps(response.json(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST 요청 예시 (데이터 전송)\n",
    "data = {\n",
    "    \"title\": \"AI 수업\",\n",
    "    \"body\": \"오늘은 LLM에 대해 배웁니다.\",\n",
    "    \"userId\": 1\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"https://jsonplaceholder.typicode.com/posts\",\n",
    "    json=data,\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "print(\"상태 코드:\", response.status_code)  # 201이면 생성 성공\n",
    "print(\"\\n응답:\")\n",
    "print(json.dumps(response.json(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Gemini API 호출 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini API 호출 함수\n",
    "def ask_llm(prompt, temperature=0.7):\n",
    "    \"\"\"\n",
    "    LLM에게 질문하는 함수\n",
    "    \n",
    "    Args:\n",
    "        prompt: 질문 내용\n",
    "        temperature: 창의성 조절 (0~1, 높을수록 창의적)\n",
    "    \n",
    "    Returns:\n",
    "        LLM의 응답 텍스트\n",
    "    \"\"\"\n",
    "    # Generation config 설정\n",
    "    generation_config = genai.GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=500\n",
    "    )\n",
    "    \n",
    "    # 시스템 프롬프트 + 사용자 프롬프트\n",
    "    full_prompt = f\"\"\"당신은 친절한 AI 어시스턴트입니다. 한국어로 답변해주세요.\n",
    "\n",
    "사용자: {prompt}\"\"\"\n",
    "    \n",
    "    response = model.generate_content(\n",
    "        full_prompt,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "print(\"LLM 질문 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 테스트\n",
    "answer = ask_llm(\"안녕하세요! 자기소개 해주세요.\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. LLM의 한계 체험하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 환각 현상 (Hallucination) 체험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 존재하지 않는 인물에 대해 질문\n",
    "fake_question = \"김철수 박사가 2023년에 발표한 '딥러닝 기반 감정 분석' 논문의 주요 내용을 설명해주세요.\"\n",
    "\n",
    "print(\"[질문]\")\n",
    "print(fake_question)\n",
    "print(\"\\n[LLM 응답]\")\n",
    "response = ask_llm(fake_question)\n",
    "print(response)\n",
    "print(\"\\n⚠️ 주의: 위 내용은 LLM이 생성한 것으로, 실제 존재하지 않을 수 있습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가짜 책에 대해 질문\n",
    "fake_book = \"'AI 시대의 윤리적 코딩'이라는 책의 저자와 출판년도를 알려주세요.\"\n",
    "\n",
    "print(\"[질문]\")\n",
    "print(fake_book)\n",
    "print(\"\\n[LLM 응답]\")\n",
    "response = ask_llm(fake_book)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 지식 단절 (Knowledge Cutoff) 체험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최신 정보 질문\n",
    "recent_question = \"오늘 날씨가 어때요?\"\n",
    "\n",
    "print(\"[질문]\")\n",
    "print(recent_question)\n",
    "print(\"\\n[LLM 응답]\")\n",
    "response = ask_llm(recent_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최신 뉴스 질문\n",
    "news_question = \"어제 있었던 주요 뉴스를 알려주세요.\"\n",
    "\n",
    "print(\"[질문]\")\n",
    "print(news_question)\n",
    "print(\"\\n[LLM 응답]\")\n",
    "response = ask_llm(news_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 복잡한 계산의 어려움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복잡한 수학 계산\n",
    "math_question = \"123456789 × 987654321을 계산해주세요.\"\n",
    "\n",
    "print(\"[질문]\")\n",
    "print(math_question)\n",
    "print(\"\\n[LLM 응답]\")\n",
    "response = ask_llm(math_question)\n",
    "print(response)\n",
    "\n",
    "# 실제 정답 확인\n",
    "print(\"\\n[실제 계산 결과]\")\n",
    "actual = 123456789 * 987654321\n",
    "print(f\"123456789 × 987654321 = {actual:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 외부 행동 불가 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부 행동 요청\n",
    "action_questions = [\n",
    "    \"네이버에서 '인공지능'을 검색해주세요.\",\n",
    "    \"제 이메일로 오늘 일정을 보내주세요.\",\n",
    "    \"현재 비트코인 가격을 알려주세요.\"\n",
    "]\n",
    "\n",
    "for q in action_questions:\n",
    "    print(f\"[질문] {q}\")\n",
    "    response = ask_llm(q)\n",
    "    print(f\"[응답] {response[:200]}...\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Temperature에 따른 응답 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature 비교 실험\n",
    "prompt = \"사과에 대해 한 문장으로 설명해주세요.\"\n",
    "\n",
    "temperatures = [0.0, 0.5, 1.0]\n",
    "\n",
    "print(f\"[프롬프트] {prompt}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n[Temperature = {temp}]\")\n",
    "    for i in range(3):  # 3번 반복해서 일관성 확인\n",
    "        response = ask_llm(prompt, temperature=temp)\n",
    "        print(f\"  시도 {i+1}: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 실습 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 1: 환각 현상 찾기\n",
    "직접 질문을 만들어 LLM의 환각 현상을 유도해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 본인만의 환각 유도 질문을 작성해보세요\n",
    "my_hallucination_question = \"여기에 질문을 입력하세요\"\n",
    "\n",
    "print(\"[내 질문]\")\n",
    "print(my_hallucination_question)\n",
    "print(\"\\n[LLM 응답]\")\n",
    "# response = ask_llm(my_hallucination_question)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 2: 대화 기록 유지하기\n",
    "여러 턴의 대화를 유지하는 함수를 완성해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 기록을 유지하는 챗봇 (Gemini 버전)\n",
    "class SimpleChatbot:\n",
    "    def __init__(self):\n",
    "        # Gemini 채팅 세션 시작\n",
    "        self.chat = model.start_chat(history=[])\n",
    "        self.messages = []\n",
    "    \n",
    "    def send_message(self, user_message):\n",
    "        # 사용자 메시지 저장\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Gemini에 메시지 전송\n",
    "        response = self.chat.send_message(user_message)\n",
    "        \n",
    "        # 응답 저장\n",
    "        assistant_message = response.text\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        return assistant_message\n",
    "    \n",
    "    def get_history(self):\n",
    "        return self.messages\n",
    "\n",
    "# 테스트\n",
    "bot = SimpleChatbot()\n",
    "print(\"챗봇이 준비되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 테스트\n",
    "print(\"[1번째 대화]\")\n",
    "response1 = bot.send_message(\"제 이름은 홍길동입니다.\")\n",
    "print(f\"봇: {response1}\\n\")\n",
    "\n",
    "print(\"[2번째 대화]\")\n",
    "response2 = bot.send_message(\"제 이름이 뭐라고 했죠?\")\n",
    "print(f\"봇: {response2}\\n\")\n",
    "\n",
    "print(\"[3번째 대화]\")\n",
    "response3 = bot.send_message(\"저에 대해 알고 있는 것을 말해주세요.\")\n",
    "print(f\"봇: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Gemini API 추가 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 사용 가능한 모델 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 가능한 모델 목록\n",
    "print(\"사용 가능한 Gemini 모델:\")\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(f\"  - {m.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 스트리밍 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍으로 실시간 응답 받기\n",
    "def ask_llm_stream(prompt):\n",
    "    \"\"\"스트리밍 방식으로 응답 받기\"\"\"\n",
    "    response = model.generate_content(prompt, stream=True)\n",
    "    \n",
    "    print(\"[실시간 응답]\")\n",
    "    for chunk in response:\n",
    "        print(chunk.text, end=\"\", flush=True)\n",
    "    print()  # 줄바꿈\n",
    "\n",
    "# 테스트\n",
    "ask_llm_stream(\"인공지능의 미래에 대해 설명해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. 토의 주제\n",
    "\n",
    "### \"AI가 스스로 생각하고 행동한다는 것은 무엇을 의미하는가?\"\n",
    "\n",
    "조별로 다음 질문들에 대해 토의해보세요:\n",
    "\n",
    "1. LLM이 \"생각\"하는 것과 인간이 \"생각\"하는 것의 차이는?\n",
    "2. Agent가 자율적으로 행동할 때, 책임은 누구에게 있는가?\n",
    "3. AI Agent에게 어느 정도의 자율성을 부여해야 할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 정리\n",
    "\n",
    "### 오늘 배운 내용\n",
    "1. **Python 기본 구조**: 변수, 자료형, 함수, 조건문, 반복문\n",
    "2. **API 호출**: HTTP 요청/응답, requests 라이브러리, Gemini API\n",
    "3. **LLM의 한계**: 환각 현상, 지식 단절, 외부 행동 불가, 계산 오류\n",
    "4. **Agentic AI의 필요성**: LLM의 한계를 극복하기 위한 방법\n",
    "\n",
    "### 사용한 API\n",
    "- **Google Gemini 2.5 Flash Lite** (무료)\n",
    "- API Key 발급: https://aistudio.google.com/app/apikey\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
