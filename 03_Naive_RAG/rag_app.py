"""
RAG ì±—ë´‡ v0.2
Gemini 2.5 Flash Lite + ChromaDB ê¸°ë°˜ ë¬¸ì„œ QA ì±—ë´‡
"""
import streamlit as st
import google.generativeai as genai
import chromadb
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

st.title("ğŸ“š RAG ì±—ë´‡ v0.2")
st.caption("ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ (Gemini + ChromaDB)")

# API í‚¤ ì„¤ì •
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except KeyError:
    api_key = st.sidebar.text_input("Gemini API Key", type="password")
    if not api_key:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì— API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

genai.configure(api_key=api_key)


# ============ í—¬í¼ í•¨ìˆ˜ë“¤ ============

def get_pdf_text(pdf_docs):
    """PDF íŒŒì¼ë“¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    text = ""
    for pdf in pdf_docs:
        try:
            reader = PdfReader(pdf)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        except Exception as e:
            st.warning(f"PDF ì½ê¸° ì˜¤ë¥˜: {e}")
    return text


def get_text_chunks(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    chunks = splitter.split_text(text)
    return chunks


def get_embedding(text):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    try:
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=text
        )
        return result['embedding']
    except Exception as e:
        st.error(f"ì„ë² ë”© ì˜¤ë¥˜: {e}")
        return None


def get_embeddings(texts):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    embeddings = []
    progress_bar = st.progress(0)
    for i, text in enumerate(texts):
        emb = get_embedding(text)
        if emb:
            embeddings.append(emb)
        progress_bar.progress((i + 1) / len(texts))
    progress_bar.empty()
    return embeddings


def create_vector_store(chunks):
    """ì²­í¬ë“¤ì„ ChromaDBì— ì €ì¥"""
    client = chromadb.Client()

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
    try:
        client.delete_collection("documents")
    except:
        pass

    collection = client.create_collection(
        name="documents",
        metadata={"hnsw:space": "cosine"}
    )

    # ì„ë² ë”© ìƒì„±
    st.info("ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = get_embeddings(chunks)

    if len(embeddings) != len(chunks):
        st.error("ì¼ë¶€ ì²­í¬ì˜ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None

    # ChromaDBì— ì €ì¥
    collection.add(
        documents=chunks,
        embeddings=embeddings,
        ids=[f"chunk_{i}" for i in range(len(chunks))],
        metadatas=[{"chunk_id": i} for i in range(len(chunks))]
    )

    return collection


def answer_question(question, collection):
    """ì§ˆë¬¸ì— ëŒ€í•œ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    # 1. ì§ˆë¬¸ ì„ë² ë”©
    q_embedding = get_embedding(question)
    if q_embedding is None:
        return "ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", None

    # 2. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    results = collection.query(
        query_embeddings=[q_embedding],
        n_results=4
    )

    if not results['documents'][0]:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", None

    # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = "\n\n---\n\n".join(results['documents'][0])

    # ë””ë²„ê¹…: ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ìœ ì‚¬ë„ ì ìˆ˜ ì €ì¥
    retrieved_docs = []
    for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):
        similarity = 1 - distance  # cosine distanceë¥¼ similarityë¡œ ë³€í™˜
        retrieved_docs.append({
            "rank": i + 1,
            "similarity": f"{similarity:.3f}",
            "preview": doc[:200] + "..." if len(doc) > 200 else doc
        })

    # 4. RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]"""

    # 5. Geminië¡œ ë‹µë³€ ìƒì„±
    try:
        model = genai.GenerativeModel("gemini-2.5-flash-lite")
        response = model.generate_content(prompt)
        return response.text, retrieved_docs
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}", retrieved_docs


# ============ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ============

if "collection" not in st.session_state:
    st.session_state.collection = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "processed" not in st.session_state:
    st.session_state.processed = False


# ============ ì‚¬ì´ë“œë°” ============

with st.sidebar:
    st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")

    pdf_docs = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        accept_multiple_files=True,
        type=['pdf']
    )

    if st.button("ğŸ“¥ ë¬¸ì„œ ì²˜ë¦¬", use_container_width=True, type="primary"):
        if pdf_docs:
            with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
                # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
                st.info("1/3: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
                raw_text = get_pdf_text(pdf_docs)

                if not raw_text.strip():
                    st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # 2. ì²­í‚¹
                    st.info("2/3: ë¬¸ì„œ ë¶„í•  ì¤‘...")
                    chunks = get_text_chunks(raw_text)

                    # 3. Vector Store ìƒì„±
                    st.info("3/3: ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
                    collection = create_vector_store(chunks)

                    if collection:
                        st.session_state.collection = collection
                        st.session_state.processed = True
                        st.success(f"âœ… {len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
        else:
            st.warning("PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")

    st.markdown("---")

    # ëŒ€í™” ì´ˆê¸°í™”
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    # ë¬¸ì„œ ì´ˆê¸°í™”
    if st.button("ğŸ“„ ë¬¸ì„œ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.collection = None
        st.session_state.processed = False
        st.session_state.messages = []
        st.rerun()

    st.markdown("---")

    # ìƒíƒœ í‘œì‹œ
    st.subheader("ğŸ“Š ìƒíƒœ")
    if st.session_state.processed:
        st.success("âœ… ë¬¸ì„œ ë¡œë“œë¨")
    else:
        st.info("â³ ë¬¸ì„œ ëŒ€ê¸° ì¤‘")

    st.write(f"ëŒ€í™” ìˆ˜: {len(st.session_state.messages)}ê°œ")


# ============ ë©”ì¸ ì±„íŒ… ì˜ì—­ ============

# ì €ì¥ëœ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        if st.session_state.collection is None:
            response = "âš ï¸ ë¨¼ì € PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
            st.warning(response)
        else:
            with st.spinner("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘..."):
                response, retrieved_docs = answer_question(prompt, st.session_state.collection)
                st.markdown(response)

                # ê²€ìƒ‰ëœ ë¬¸ì„œ í‘œì‹œ (ë””ë²„ê¹…ìš©)
                if retrieved_docs:
                    with st.expander("ğŸ” ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ (ë””ë²„ê¹…)"):
                        for doc in retrieved_docs:
                            st.markdown(f"**#{doc['rank']}** (ìœ ì‚¬ë„: {doc['similarity']})")
                            st.caption(doc['preview'])
                            st.markdown("---")

        st.session_state.messages.append({"role": "assistant", "content": response})
